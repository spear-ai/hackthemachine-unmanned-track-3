{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electric-linux",
   "metadata": {},
   "source": [
    "### Grid World with Q-learning Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-drinking",
   "metadata": {},
   "source": [
    "The agent-environment interaction\n",
    "\n",
    "The Agent uses Q-learning with initial Q values of 0, a stochasticity parameter for the $\\epsilon$-greedy policy function $\\epsilon=0.05$, and a learning rate $\\alpha = 0.1$.\n",
    "\n",
    "The interaction of the agent with its environments starts at decision stage $t=0$ with the observation of the current state $s_0$. (Notice that there is no reward at this initial stage). The agent then chooses an action to execute at decision stage $t=1$. The environment responds by changing its state to $s_1$ and returning the numerical reward signal $r_1$.\n",
    "\n",
    "##### The environment: Navigation in a gridworld\n",
    "\n",
    "\n",
    "The agent has four possible actions in each state (grid square): cardinal points (NESW). The actions are unreliable. The ACTION moves the agent in the intended direction with probability 0.8, and in a random alternative direction with probability 0.2. If the direction of movement is blocked, the agent remains in the same grid square.\n",
    "\n",
    "A reinforcement learning algorithm is used to compute the best policy for reaching the goal with as few steps as possible while avoiding the predator(s). For this, we will use the following reward function: $-1$ for each navigation action, an additional $+10$ for reaching the goal, and an additional $-15$ for hitting a predator. For example, the immediate reward for transitioning into the goal square is $-1 + 10 = +9$. Do not use discounting (that is, set $\\gamma=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-profile",
   "metadata": {},
   "source": [
    "###### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "loose-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import operator\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-original",
   "metadata": {},
   "source": [
    "###### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-manufacturer",
   "metadata": {},
   "source": [
    "###### Read in a pre-made 24 x 24 cell gameboard (centre-points of each cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "formed-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_board_gpd = gpd.read_file(\"GameBoard_4326_centrepoints.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "swedish-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _board_loc(bread_crumb):\n",
    "    \"\"\"\n",
    "    Return the lat lon point given the numpy board location\n",
    "    Args:\n",
    "        bread_crumb: tuple (4,2)\n",
    "    Return:\n",
    "        Point\n",
    "    \"\"\"\n",
    "    game_board_id = (bread_crumb[0] +1) + (bread_crumb[1] * 24)\n",
    "    pt_gpd = game_board_gpd.loc[game_board_gpd[\"id\"] == game_board_id]\n",
    "    pt = [(x,y) for x, y in zip(pt_gpd.geometry.x, pt_gpd.geometry.y)]\n",
    "    return Point(pt[0])\n",
    "    \n",
    "# test\n",
    "#print(_board_loc([(4,2), (5,5)])) \n",
    "#print(_board_loc((4,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-charge",
   "metadata": {},
   "source": [
    "#### Create Track\n",
    "- From base geojson of 24 x 24 centre-points get the location of the id that corresponds to the bread crumb\n",
    "- geojson top left point is id 1, bottom right is id 576\n",
    "- Bread crumb conversion example:\n",
    "- bread crumb == (4, 2)\n",
    "- bread crumb to id: ((4 +1) + (2 x 24)) = ID 53\n",
    "- write list of crumbs to geojson with timestamp column of 1 hour interval so that Jackson can simulate movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "shaped-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create track\n",
    "def _track(bread_crumb_list, pts_game_board_gpd):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        bread_crumb_trail: list of tuples of locations on numpy array gameboard\n",
    "        pts_game_board_gpd: Geopandas dataframe of points of the game board\n",
    "                            centrepoints\n",
    "    Returns:\n",
    "        geopandas dataframe of points of the track\n",
    "        with timestamp column to simulate movement over time (1 hour increments) \n",
    "    \"\"\"\n",
    "    geom = [_board_loc(crumb) for crumb in bread_crumb_list]\n",
    "    now = datetime.now()\n",
    "    times = [(now + timedelta(hours=dt)).strftime(\"%Y-%m-%d-%H:%M:%S\") for dt in range(len(bread_crumb_list))]\n",
    "    crumb_ids = [i for i in range(len(bread_crumb_list))]\n",
    "    track_gpd = gpd.GeoDataFrame({\"geometry\": geom, \"time\": times, \"crumb_id\": crumb_ids}, crs=4326)\n",
    "    return track_gpd\n",
    "\n",
    "\n",
    "# test\n",
    "#test_crumbs = [(23,23), (23,22), (23,21)]\n",
    "#track = _track(test_crumbs, game_board_gpd)\n",
    "#print(track.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "killing-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _id_to_grid_place(grid_id, grid_size=24):\n",
    "    \"\"\"\n",
    "    Return the numpy board location given the dataframe grid id\n",
    "    e.g.: ID 53 in the geodataframe = (4,2)\n",
    "    Args:\n",
    "        grid_id: int, gpd grid id \n",
    "        grid_size: int, size of the numpy grid (same as gpd grid)\n",
    "    Return:\n",
    "        Tuple (4,2) location of gpd id on 24 x 24 numpy array\n",
    "    \"\"\"\n",
    "    gid = grid_id - 1\n",
    "    numpy_id =  (gid - (math.floor(gid / grid_size) * grid_size), math.floor((gid / grid_size))) \n",
    "    return numpy_id\n",
    "    \n",
    "# test\n",
    "# print(_id_to_grid_place(53))\n",
    "# print(_id_to_grid_place(1))\n",
    "# print(_id_to_grid_place(576))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "corresponding-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _masked_array(game_board_gpd, grid_size=24):\n",
    "    \"\"\"\n",
    "    Create a mask on the grid with value -1 for all cells that are land.\n",
    "    N.B.: This doesn't return a numpy masked array, rather the cell has a -1 penalty,\n",
    "          this is a flaw but serves well enough to represent the path of the smugglers\n",
    "          for visualization and agent behaviour understanding.\n",
    "    Args:\n",
    "        game_board_gpd: GeoDataFrame of points\n",
    "        grid_size: int, default 24\n",
    "    Returns:\n",
    "        list of tuples representing the locations of land cells\n",
    "        \n",
    "    \"\"\"\n",
    "    mask_gpd = game_board_gpd[game_board_gpd.Mask == 1]\n",
    "    mask_locations = [_id_to_grid_place(gpd_id) for gpd_id in mask_gpd.id]\n",
    " \n",
    "    # masked numpy array\n",
    "    base_grid = np.zeros((grid_size, grid_size))\n",
    "    _mask = np.zeros((grid_size, grid_size))\n",
    "    for mask_loc in mask_locations:\n",
    "        _mask[mask_loc[0], mask_loc[1]] = -1\n",
    "    \n",
    "    #masked_grid = ma.array(base_grid, mask =_mask)\n",
    "    #return masked_grid[~masked_grid.mask]\n",
    "    \n",
    "    return _mask\n",
    "\n",
    "#print(_masked_array(game_board_gpd, grid_size=24))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-swedish",
   "metadata": {},
   "source": [
    "###### Game board environment: Grid World Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "opened-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    ## Initialise starting data\n",
    "    def __init__(self, grid_height=24, grid_width=24, pred_loc=[(2,3), (14,16)], goal_loc=(0,0), start_loc=(23,17)):\n",
    "        # Set information about the gridworld\n",
    "        self.height = grid_height\n",
    "        self.width = grid_width\n",
    "        self.grid = np.zeros(( self.height, self.width)) - 1\n",
    "        self.grid = _masked_array(game_board_gpd)\n",
    "        \n",
    "        # Set start location\n",
    "        self.current_location = start_loc\n",
    "        \n",
    "        # Set locations for the predators and the goal\n",
    "        self.pred_location = pred_loc\n",
    "        self.goal_location = goal_loc\n",
    "        self.terminal_states = self.pred_location + [self.goal_location]\n",
    "        \n",
    "        # Set grid rewards for special cells\n",
    "        for p in self.pred_location:\n",
    "            self.grid[p[0], p[1]] = -15\n",
    "        self.grid[ self.goal_location[0], self.goal_location[1]] = 10\n",
    "        \n",
    "        # Set available actions\n",
    "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "        \n",
    "    def get_available_actions(self):\n",
    "        \"\"\"Returns possible actions\"\"\"\n",
    "        return self.actions\n",
    "    \n",
    "    def agent_on_map(self):\n",
    "        \"\"\"Prints out current location of the agent on the grid (used for debugging)\"\"\"\n",
    "        grid = np.zeros(( self.height, self.width))\n",
    "        grid[ self.current_location[0], self.current_location[1]] = 1\n",
    "        return grid\n",
    "    \n",
    "    def get_reward(self, new_location):\n",
    "        \"\"\"Returns the reward for an input position\"\"\"\n",
    "        return self.grid[ new_location[0], new_location[1]]\n",
    "        \n",
    "    \n",
    "    def make_step(self, action):\n",
    "        \"\"\"Moves the agent in the specified direction. If agent is at a border, agent stays still\n",
    "        but takes negative reward. Function returns the reward for the move.\"\"\"\n",
    "        # Store previous location\n",
    "        last_location = self.current_location\n",
    "        \n",
    "        # UP\n",
    "        if action == 'UP':\n",
    "            # If agent is at the top, stay still, collect reward\n",
    "            if last_location[0] == 0:\n",
    "                reward = self.get_reward(last_location)\n",
    "            else:\n",
    "                self.current_location = ( self.current_location[0] - 1, self.current_location[1])\n",
    "                reward = self.get_reward(self.current_location)\n",
    "        \n",
    "        # DOWN\n",
    "        elif action == 'DOWN':\n",
    "            # If agent is at bottom, stay still, collect reward\n",
    "            if last_location[0] == self.height - 1:\n",
    "                reward = self.get_reward(last_location)\n",
    "            else:\n",
    "                self.current_location = ( self.current_location[0] + 1, self.current_location[1])\n",
    "                reward = self.get_reward(self.current_location)\n",
    "            \n",
    "        # LEFT\n",
    "        elif action == 'LEFT':\n",
    "            # If agent is at the left, stay still, collect reward\n",
    "            if last_location[1] == 0:\n",
    "                reward = self.get_reward(last_location)\n",
    "            else:\n",
    "                self.current_location = ( self.current_location[0], self.current_location[1] - 1)\n",
    "                reward = self.get_reward(self.current_location)\n",
    "\n",
    "        # RIGHT\n",
    "        elif action == 'RIGHT':\n",
    "            # If agent is at the right, stay still, collect reward\n",
    "            if last_location[1] == self.width - 1:\n",
    "                reward = self.get_reward(last_location)\n",
    "            else:\n",
    "                self.current_location = ( self.current_location[0], self.current_location[1] + 1)\n",
    "                reward = self.get_reward(self.current_location)\n",
    "                \n",
    "        return reward\n",
    "    \n",
    "    def check_state(self):\n",
    "        \"\"\"Check if the agent is in a terminal state (goal or predator), if so return 'TERMINAL'\"\"\"\n",
    "        if self.current_location in self.terminal_states:\n",
    "            return 'TERMINAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "demographic-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():        \n",
    "    # Choose a random action\n",
    "    def choose_action(self, available_actions):\n",
    "        \"\"\"Returns a random choice of the available actions\"\"\"\n",
    "        return np.random.choice(available_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "complete-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Agent():\n",
    "    # Intialise\n",
    "    def __init__(self, environment, epsilon=0.05, alpha=0.1, gamma=1):\n",
    "        self.environment = environment\n",
    "        self.q_table = dict() # Store all Q-values in dictionary of dictionaries \n",
    "        for x in range(environment.height): # Loop through all possible grid spaces, create sub-dictionary for each\n",
    "            for y in range(environment.width):\n",
    "                self.q_table[(x,y)] = {'UP':0, 'DOWN':0, 'LEFT':0, 'RIGHT':0} # Populate sub-dictionary with zero values for possible moves\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def choose_action(self, available_actions):\n",
    "        \"\"\"Returns the optimal action from Q-Value table. If multiple optimal actions, chooses random choice.\n",
    "        Will make an exploratory random action dependent on epsilon.\"\"\"\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = available_actions[np.random.randint(0, len(available_actions))]\n",
    "        else:\n",
    "            q_values_of_state = self.q_table[self.environment.current_location]\n",
    "            maxValue = max(q_values_of_state.values())\n",
    "            action = np.random.choice([k for k, v in q_values_of_state.items() if v == maxValue])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self, old_state, reward, new_state, action):\n",
    "        \"\"\"Updates the Q-value table using Q-learning\"\"\"\n",
    "        q_values_of_state = self.q_table[new_state]\n",
    "        max_q_value_in_new_state = max(q_values_of_state.values())\n",
    "        current_q_value = self.q_table[old_state][action]\n",
    "        \n",
    "        self.q_table[old_state][action] = (1 - self.alpha) * current_q_value + self.alpha * (reward + self.gamma * max_q_value_in_new_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "tired-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(environment, agent, trials=500, max_steps_per_episode=1000, learn=False):\n",
    "    \"\"\"The play function runs iterations and updates Q-values if desired.\"\"\"\n",
    "    reward_per_episode = [] # Initialise performance log\n",
    "    bread_crumbs_per_episode = []\n",
    "    \n",
    "    for trial in range(trials): # Run trials\n",
    "        cumulative_reward = 0 # Initialise values of each game\n",
    "        step = 0\n",
    "        game_over = False\n",
    "        crumbs = []\n",
    "        while step < max_steps_per_episode and game_over != True: # Run until max steps or until game is finished\n",
    "            old_state = environment.current_location\n",
    "            action = agent.choose_action(environment.actions) \n",
    "            reward = environment.make_step(action)\n",
    "            new_state = environment.current_location\n",
    "            \n",
    "            if learn == True: # Update Q-values if learning is specified\n",
    "                agent.learn(old_state, reward, new_state, action)\n",
    "                \n",
    "            cumulative_reward += reward\n",
    "            step += 1\n",
    "            #crumbs.append(environment.agent_on_map())\n",
    "            crumbs.append(environment.current_location)\n",
    "            \n",
    "            if environment.check_state() == 'TERMINAL': # If game is in terminal state, game over and start next trial\n",
    "                environment.__init__()\n",
    "                game_over = True     \n",
    "                \n",
    "        reward_per_episode.append(cumulative_reward) # Append reward for current trial to performance log\n",
    "        bread_crumbs_per_episode.append(crumbs)\n",
    "        \n",
    "    return reward_per_episode, bread_crumbs_per_episode # Return performance log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-durham",
   "metadata": {},
   "source": [
    "#### Run Random Agent\n",
    "- Random agent moves randomly and does not learn from it's actions.\n",
    "- This gives a base performance to compare the Q-Learning agent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "opponent-testament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current position of the agent = (23, 17)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "Available_actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
      "Randomly chosen action = DOWN\n",
      "Reward obtained = 0.0\n",
      "Current position of the agent = (23, 17)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(grid_height=24, grid_width=24, pred_loc=[(2,2)], goal_loc=(0,0), start_loc=(23, 17))\n",
    "agent = RandomAgent()\n",
    "\n",
    "print(\"Current position of the agent =\", env.current_location)\n",
    "print(env.agent_on_map())\n",
    "available_actions = env.get_available_actions()\n",
    "print(\"Available_actions =\", available_actions)\n",
    "chosen_action = agent.choose_action(available_actions)\n",
    "print(\"Randomly chosen action =\", chosen_action)\n",
    "reward = env.make_step(chosen_action)\n",
    "print(\"Reward obtained =\", reward)\n",
    "print(\"Current position of the agent =\", env.current_location)\n",
    "print(env.agent_on_map())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-memphis",
   "metadata": {},
   "source": [
    "#### Q-Agent\n",
    "- Exeriment with the number of trials until the learning rate converges\n",
    "- Performance is plotted\n",
    "- Add predators to see how the agent's path changes\n",
    "- Change the goal location to see how the agent's path changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "shared-cooking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc4a1a0bfd0>]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GridWorld Environment\n",
    "environment = GridWorld(grid_height=24, grid_width=24, pred_loc=[(2,2), (14, 16)], goal_loc=(0,0), start_loc=(23, 17))\n",
    "agentQ = Q_Agent(environment)\n",
    "\n",
    "# Note the learn=True argument!\n",
    "reward_per_episode, bread_crumbs_per_episode = play(environment, agentQ, trials=20000, learn=True)\n",
    "\n",
    "# Simple learning curve\n",
    "plt.plot(reward_per_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "stylish-mexican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 18)\n",
      "(23, 17)\n",
      "(22, 17)\n",
      "(22, 16)\n",
      "(22, 15)\n",
      "(23, 15)\n",
      "(23, 14)\n",
      "(23, 14)\n",
      "(23, 13)\n",
      "(23, 12)\n",
      "(22, 12)\n",
      "(22, 11)\n",
      "(22, 10)\n",
      "(21, 10)\n",
      "(21, 9)\n",
      "(20, 9)\n",
      "(20, 8)\n",
      "(20, 7)\n",
      "(19, 7)\n",
      "(18, 7)\n",
      "(17, 7)\n",
      "(16, 7)\n",
      "(16, 6)\n",
      "(15, 6)\n",
      "(14, 6)\n",
      "(13, 6)\n",
      "(12, 6)\n",
      "(11, 6)\n",
      "(11, 5)\n",
      "(11, 4)\n",
      "(10, 4)\n",
      "(10, 3)\n",
      "(10, 2)\n",
      "(9, 2)\n",
      "(9, 1)\n",
      "(8, 1)\n",
      "(7, 1)\n",
      "(6, 1)\n",
      "(6, 0)\n",
      "(5, 0)\n",
      "(4, 0)\n",
      "(3, 0)\n",
      "(2, 0)\n",
      "(1, 0)\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "# print the coordinates for the last episode\n",
    "for b in bread_crumbs_per_episode[-1]:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "headed-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the last track to geojson (this one should have the best learned route)\n",
    "track = _track(bread_crumbs_per_episode[-1], game_board_gpd)\n",
    "test_track = \"test_track.geojson\"  \n",
    "track.to_file(test_track, driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-amount",
   "metadata": {},
   "source": [
    "##### Write a series of geojsons that represent a progression of the agent learning\n",
    "- Edit the choice of trails as desired\n",
    "- Default 1st, 10% increments, last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "premier-decline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000, 4000, 6000, 8000, 10000, 12000, 14000, 16000, 18000, 0, 19999]\n"
     ]
    }
   ],
   "source": [
    "# write track series to geojsons\n",
    "# output_dir = \"path_to/rl_output_dir\"\n",
    "\n",
    "# get bread_crumb trail at 10% increments\n",
    "increments = np.arange(0.1, 1, 0.1)\n",
    "selected_crumbs = [round(len(bread_crumbs_per_episode)*i) for i in increments]\n",
    "# add first trail to the list\n",
    "selected_crumbs.append(0)\n",
    "# add last trail to the list\n",
    "selected_crumbs.append(len(bread_crumbs_per_episode) - 1)\n",
    "\n",
    "\n",
    "for _trail in selected_crumbs:\n",
    "    output_file = f\"track_{_trail}.geojson\"\n",
    "    track = _track(bread_crumbs_per_episode[_trail], game_board_gpd)\n",
    "    # output_file_path = os.path.join(output_dir, output_file) \n",
    "    track.to_file(output_file, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-currency",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
